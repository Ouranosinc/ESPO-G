Building DAG of jobs...
Using shell: /cvmfs/soft.computecanada.ca/gentoo/2023/x86-64-v3/usr/bin/bash
Provided remote nodes: 1
Provided resources: mem_mb=50000, mem_mib=47684, disk_mb=1000, disk_mib=954
Select jobs to execute...
Execute 1 jobs...

[Thu Jul 18 17:16:02 2024]
rule extract:
    output: /scratch/oumou/ESPO-G6-SNAKEMAKE/ESPO-G_workdir/CMIP6_ScenarioMIP_AS-RCEC_TaiESM1_ssp585_r1i1p1f1_global_north_nodup_extracted.zarr
    log: logs/extract_CMIP6_ScenarioMIP_AS-RCEC_TaiESM1_ssp585_r1i1p1f1_global_north_nodup
    jobid: 0
    reason: Forced execution
    wildcards: sim_id=CMIP6_ScenarioMIP_AS-RCEC_TaiESM1_ssp585_r1i1p1f1_global, region=north_nodup
    threads: 10
    resources: mem_mb=50000, mem_mib=47684, disk_mb=1000, disk_mib=954, tmpdir=<TBD>, slurm_partition=c-frigon, runtime=120, constraint=genoa, slurm_account=ctb-frigon_cpu

Building DAG of jobs...
Using shell: /cvmfs/soft.computecanada.ca/gentoo/2023/x86-64-v3/usr/bin/bash
Provided cores: 10
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=50000, mem_mib=47684, disk_mb=1000, disk_mib=954
Select jobs to execute...
Execute 1 jobs...

[Thu Jul 18 17:16:09 2024]
localrule extract:
    output: /scratch/oumou/ESPO-G6-SNAKEMAKE/ESPO-G_workdir/CMIP6_ScenarioMIP_AS-RCEC_TaiESM1_ssp585_r1i1p1f1_global_north_nodup_extracted.zarr
    log: logs/extract_CMIP6_ScenarioMIP_AS-RCEC_TaiESM1_ssp585_r1i1p1f1_global_north_nodup
    jobid: 0
    reason: Forced execution
    wildcards: sim_id=CMIP6_ScenarioMIP_AS-RCEC_TaiESM1_ssp585_r1i1p1f1_global, region=north_nodup
    threads: 10
    resources: mem_mb=50000, mem_mib=47684, disk_mb=1000, disk_mib=954, tmpdir=/tmp, slurm_partition=c-frigon, runtime=120, constraint=genoa, slurm_account=ctb-frigon_cpu

*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[nc31201.narval.calcul.quebec:103122] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
RuleException:
CalledProcessError in file /home/oumou/espo_SNAKEMAKE_NARVAL/workflow/rules/extract.smk, line 19:
Command 'set -euo pipefail;  /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/bin/python /home/oumou/espo_SNAKEMAKE_NARVAL/.snakemake/scripts/tmpr52syvhu.extract.py' returned non-zero exit status 1.
[Thu Jul 18 17:16:12 2024]
Error in rule extract:
    jobid: 0
    output: /scratch/oumou/ESPO-G6-SNAKEMAKE/ESPO-G_workdir/CMIP6_ScenarioMIP_AS-RCEC_TaiESM1_ssp585_r1i1p1f1_global_north_nodup_extracted.zarr
    log: logs/extract_CMIP6_ScenarioMIP_AS-RCEC_TaiESM1_ssp585_r1i1p1f1_global_north_nodup (check log file(s) for error details)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Storing output in storage.
WorkflowError:
At least one job did not complete successfully.
slurmstepd: error:  mpi/pmix_v4: _errhandler: nc31201 [0]: pmixp_client_v2.c:212: Error handler invoked: status = -61, source = [slurm.pmix.31714782.0:0]
slurmstepd: error: *** STEP 31714782.0 ON nc31201 CANCELLED AT 2024-07-18T21:16:13 ***
srun: Job step aborted: Waiting up to 62 seconds for job step to finish.
srun: error: nc31201: task 0: Killed
[Thu Jul 18 17:16:13 2024]
Error in rule extract:
    jobid: 0
    output: /scratch/oumou/ESPO-G6-SNAKEMAKE/ESPO-G_workdir/CMIP6_ScenarioMIP_AS-RCEC_TaiESM1_ssp585_r1i1p1f1_global_north_nodup_extracted.zarr
    log: logs/extract_CMIP6_ScenarioMIP_AS-RCEC_TaiESM1_ssp585_r1i1p1f1_global_north_nodup (check log file(s) for error details)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Storing output in storage.
WorkflowError:
At least one job did not complete successfully.
